{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TracHack Submission Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier,NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a submission in TracHack, you will need to modify the select_data, featurize_data, and train_model functions.  All other functions should not be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,ArrayType,IntegerType,FloatType,DoubleType,TimestampType,DateType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flatten(schema, prefix=None):\n",
    "    fields = []\n",
    "    for field in schema.fields:\n",
    "        name = prefix + '.' + field.name if prefix else field.name\n",
    "        dtype = field.dataType\n",
    "        if isinstance(dtype, ArrayType):\n",
    "            dtype = dtype.elementType\n",
    "\n",
    "        if isinstance(dtype, StructType):\n",
    "            fields += flatten(dtype, prefix=name)\n",
    "        else:\n",
    "            fields.append(name)\n",
    "\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getsum(array):\n",
    "  count = 0.0\n",
    "  for x in array:\n",
    "    count += x\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def time_diff(array1,array2):\n",
    "  \n",
    "  #first convert element in array to date format\n",
    "  from datetime import datetime\n",
    "  start = [datetime.strptime(x,\"%Y-%m-%d\") for x in array1]\n",
    "  end = [datetime.strptime(y,\"%Y-%m-%d\") for y in array2]\n",
    "  diff = [b-a for a,b in zip(start,end)] # get the time difference\n",
    "  diff = [i.days for i in diff]# convert timedelta to int\n",
    "  return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def select_data(data_frame):\n",
    "    \"\"\"Selects and transforms the raw GCR (JSON) records data frame into a data frame.\n",
    "\n",
    "    :param data_frame: Input data frame to select the specific data elements from.\n",
    "    :returns selected dataframe that includes the columns of interest.\n",
    "    \"\"\"\n",
    "    u = udf(getsum, DoubleType())\n",
    "    time_duration = udf(time_diff,ArrayType(IntegerType()))\n",
    "\n",
    "    df=data_frame.select(flatten(data_frame.schema))\n",
    "    df=df.withColumn('activation_channel',df['activation_channel'].getItem(0)) \\\n",
    "    .withColumn('activation_date',to_date(df['activation_date'].getItem(0),\"yyyy-MM-dd\")) \\\n",
    "    .withColumn('first_activation_date',to_date(df['first_activation_date'],\"yyyy-MM-dd\")) \\\n",
    "    .withColumn('birth_year',df.birth_year.cast(\"integer\"))\n",
    "    \n",
    "    df=df.withColumn('days_from_activation_date',datediff(to_date(lit(\"2020-03-19\")),df.activation_date)) \\\n",
    "    .withColumn(\"TotalDataUsage\",u(df.data)) \\\n",
    "    .withColumn(\"sus_diff_day\",time_duration(df.start_date,df.end_date))\n",
    "    \n",
    "    df_selection = df.select('status','manufacturer', 'retailer', 'model','volte'\n",
    "                             ,\"lifetime_redemptions\", \"lifetime_revenues\")\n",
    "#    ,'birth_year','lifetime_redemptions','lifetime_revenues','TotalDataUsage'\n",
    "    # Annotate missing values with 'unknown' for the fields that we are interested in.\n",
    "    df_selection = df_selection.fillna({'manufacturer': \"unknown\", 'retailer': \"unknown\", \"model\": \"unknown\",\"volte\": \"unknown\"})\n",
    "    return df_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def featurize_data(df, remove_orig_cols=True):\n",
    "    \"\"\"Given a selected data frame, generate a featurized dataframe.\n",
    "\n",
    "    Example: Shows how categorical features are handled by first a string indexer and then one-hot encoding.\n",
    "\n",
    "    if remove_orig_cols is set, only returns a dataframe with two columns - features and label.\n",
    "\n",
    "\n",
    "    :param df: Input data frame to featurize.\n",
    "    :param remove_orig_cols: (Default=True) If set we remove the original columns from the returned dataframe.\n",
    "    :returns featurized dataframe that includes two columns - label and features. If remove_orig_cols=False, includes\n",
    "    the columns from original input data frame as well.\n",
    "    \"\"\"\n",
    "\n",
    "    stages = []\n",
    "\n",
    "    categorical_columns = ['manufacturer', 'retailer', 'model','volte']\n",
    "    for column in categorical_columns:\n",
    "        string_indexer = StringIndexer(inputCol=column, outputCol=column + '_index')\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[string_indexer.getOutputCol()], outputCols=[column + \"_vec\"])\n",
    "        stages += [string_indexer, encoder]\n",
    "\n",
    "    label_indexer = StringIndexer(inputCol=\"status\", outputCol=\"label\")\n",
    "    stages += [label_indexer]\n",
    "\n",
    "    numeric_columns = [\"lifetime_redemptions\", \"lifetime_revenues\"]\n",
    "\n",
    "    assembler_inputs = [c + \"_vec\" for c in categorical_columns] + numeric_columns\n",
    "    assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "    stages += [assembler]\n",
    "\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    df_featurized = pipeline_model.transform(df)\n",
    "    if remove_orig_cols:\n",
    "        selected_columns = ['features', 'label']\n",
    "        return df_featurized.select(selected_columns)\n",
    "    else:\n",
    "        return df_featurized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_model(train_dataset):\n",
    "    \"\"\"Given a featurized training dataset, trains a simple logistic regression model and\n",
    "    returns the trained model object.\n",
    "\n",
    "    :param train_dataset: A dataframe with two columns - label and features.\n",
    "    :returns trained model object.\n",
    "    \"\"\"\n",
    "#     dtc = DecisionTreeClassifier(labelCol='label',featuresCol='features')\n",
    "    gbc = GBTClassifier(labelCol='label',featuresCol='features')\n",
    "#     rfc = RandomForestClassifier(labelCol='label',featuresCol='features')\n",
    "#     nbc = NaiveBayes(labelCol='label',featuresCol='features')\n",
    "#     lr  = LogisticRegression(featuresCol='features', labelCol='label', maxIter=15)\n",
    "    \n",
    "#     dtc_model = dtc.fit(train_dataset)\n",
    "    gbc_model = gbc.fit(train_dataset)\n",
    "#     rfc_model = rfc.fit(train_dataset)\n",
    "#     nbc_model = nbc.fit(train_dataset)\n",
    "#     nbc_model = nbc.fit(train_dataset)\n",
    "#     lr_recipe = LogisticRegression(featuresCol='features', labelCol='label', maxIter=15)\n",
    "#     lr_model = lr_recipe.fit(train_dataset)\n",
    "    return gbc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_model(model, test_dataset):\n",
    "    \"\"\"Given a model and featurized test dataset, returns the auc value.\n",
    "\n",
    "    :param model: the pyspark.ml model object to evaluate.\n",
    "    :param test_dataset: A dataframe with two columns - label and features, to evaluate the model.\n",
    "    :returns AUC under ROC value.\n",
    "\n",
    "    \"\"\"\n",
    "    predictions = model.transform(test_dataset)\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as sum_\n",
    "def trachack_submission(spark, data_path, random_seed, test_ratio=0.2, num_folds=10):\n",
    "    \"\"\"The end to end model pipeline, printing out the AUC value for each fold and the averaged AUC value.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param data_path: path to the dataset to use for the pipeline.\n",
    "    :param random_seed: seed as input to the seed selection for randomSplit for train/test split.\n",
    "    :param test_ratio: (Default=0.2) Percentage of data to use for test/evaluation. Must be between 0 and 1.\n",
    "    :param num_folds: (Default=10) Number of folds for averaged AUC value.\n",
    "    :returns: averaged AUC value\n",
    "    \"\"\"\n",
    "    df = spark.read.json(data_path)\n",
    "#     df = spark.read.parquet(data_path)\n",
    "    \n",
    "    df_selected = select_data(df)\n",
    "    df_featurized = featurize_data(df_selected)\n",
    "\n",
    "    # Cache this data frame since we will be doing multiple passes to split, train and evaluate\n",
    "    df_featurized.cache()\n",
    "    \n",
    "    fold_auc = []\n",
    "    for i in range(num_folds):\n",
    "        fold_seed = random_seed * i\n",
    "        train, test = df_featurized.randomSplit([1.0 - test_ratio, test_ratio], seed=fold_seed)\n",
    "        \n",
    "        model = train_model(train)\n",
    "        auc = evaluate_model(model, test)\n",
    "        print(f\"Fold {i} AUC: {auc}\")\n",
    "        fold_auc.append(auc)\n",
    "\n",
    "    average_auc = __builtin__.sum(fold_auc) / num_folds\n",
    "    print(f\"Average AUC: {average_auc}\")\n",
    "    return average_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get results\n",
    "trachack_submission(spark, df_path, 123)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "trachack-submission",
  "notebookId": 2299079122522717
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
